extensions[matrix]

globals [
  episodes    ;; number of episodes for test
  steps       ;; number of steps per episode
  gamma       ;; discount factor
  lr          ;; learning rate
  er          ;; exploration rate
  quality     ;; quality matrix
  actions     ;; number of actions of an agent
  states      ;; number of states of an agent
  curr-state  ;; current state
]

to reset-episode
  set gamma 0.8
  set lr 1
  set episodes 1000
  set steps 1000
  
  set states 33333312
  set actions 2
  
  set curr-state (list 0 0 0 0)
  
  set quality matrix:make-constant states actions 0  ;; init matrix with zeros
end


to run-episode [update-function stop-function]
  show "test functions get-index-from-state and get-state-from-index" 
  show (list -16 -12 2 -8)
  show get-index-from-state (list -16 -12 2 -8) 
  show get-state-from-index 40208
  
  repeat steps [
    let action choose-action curr-state
    
    (run update-function action)  ;; callback to update the graphics
    
    let new-state perform-step action
    
    let reward (get-reward new-state action)
    
    ;; Q(s,a) := Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
    ;; Q(s,a) := R(s,a) + gamma * max Q(s',a') simple version for now
    let state-index (get-index-from-state new-state)
    let new-quality (reward + gamma * max (matrix:get-row quality state-index))
    matrix:set quality curr-state action new-quality
    
    ;; show new-quality
    
    set curr-state new-state
  ]
  run stop-function
end


to-report perform-step [action]
  let next-state curr-state  ;; copy current state
  let paddle-x (item 3 next-state)
  
  set paddle-x paddle-x + (ifelse-value action = 0 [-1] [1])
  
  report replace-item 3 next-state paddle-x  ;; replace with the new paddle position
end


to-report get-best-action [state]  
  ;; get quality values for each action given the current state
  let state-index (get-index-from-state state)
  let row matrix:get-row quality state-index
  
  ;; return the action with max quality
  report ifelse-value (item 0 row > item 1 row) [0] [1]
end


to-report choose-action [state]
  if random 2 < er [
    report get-best-action state
  ]
  report int(random 2)
end



;; TODO: aggiustare ball-angle in base al range di valori
to-report get-index-from-state [state]
  let ball-x (item 0 state)
  let ball-y (item 1 state)
  let ball-angle (item 2 state)
  let paddle-x (item 3 state)
  
  report (ball-x + max-pxcor) * 1000000 + (ball-y + max-pycor) * 10000 + ball-angle * 100 + (paddle-x + max-pxcor)
end


;; TODO: aggiustare ball-angle in base al range di valori
to-report get-state-from-index [index]
  let paddle-x (index mod 100)
  let ball-angle ((index mod 10000) - paddle-x) / 100
  let ball-y ((((index mod 1000000) - paddle-x) / 100) - ball-angle) / 100
  let ball-x (((((index mod 100000000) - paddle-x) / 100) - ball-angle) / 100 - ball-y) / 100
  
  set paddle-x paddle-x - max-pxcor
  set ball-y ball-y - max-pxcor
  set ball-x ball-x - max-pxcor
  
  report (list ball-x ball-y ball-angle paddle-x)
end


to-report get-reward [state action]
  let ball-x (item 0 state)
  let ball-y (item 1 state)
  let paddle-x (item 3 state)
  
  if ball-x != paddle-x [
    if ball-y = min-pycor  ;; player 1 loses
        [report 1]         ;; TODO: aggiustare in base a quale player impara
    
    if ball-y = max-pycor  ;; player 2 loses
        [report -1]        ;; TODO: aggiustare in base a quale player impara
  ]
  
  report 0  ;; nothing happens
end


