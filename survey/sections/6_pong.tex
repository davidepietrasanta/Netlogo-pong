\section{Learning Pong}
After papers like "Reward is enough"\cite{silver2021reward} and "Playing Atari with Deep Reinforcement Learning"\cite{mnih2013playing} the enthusiasm for the RL has risen more and more.
"Reward is enough" hypothesizes how intelligence can be understood as subservient to reward maximization. The reward is enough to drive behavior that displays skills studied in natural and artificial intelligence. This is in contrast to the idea that specialized problem formulations, based on other signals or goals, are required for each skill \cite{silver2021reward}.

Mnih, Volodymyr, et al. \cite{mnih2013playing} introduced the concept of Deep Q-Network, a convolutional neural network, trained with a variant of Q-learning.
Seven popular ATARI games were considered in this work, including pong.
%
Grayscale game frames are captured at 210x160 resolution, downsampled to a 110x84 image and cropped to an 84x84 patch.
%
The network has an input shape of $84\times 84\times k$, where $k$ is the number of skipped frame and has a separate output unit for each possible action, 
the number of which depends on the game considered. 
For all games considered all positive rewards are fixed at 1 and all negative rewards at -1.
%
The network is trained with an $\epsilon$-greedy annealed strategy and a replay memory of most recent frames is used to ease the training.

Another work that uses DQN focusing on pong is \cite{makarov2017learning}, which use an episodic control technique \cite{blundell2016model}.
%
The reward considered is the same as in "Playing Atari with Deep Reinforcement Learning" +1 when the agent score a point -1 when the opponent score a point.
The screenshots with a fixed 32 FPS are binarized and rescaled to 80x80.
Agent actions considered are move paddle up, move paddle down, paddle stays at the same place.
In particular they added an episodic control technique, implemented with an embedding function represented as a random projection, to reuse successful strategies.


Diallo et al. \cite{diallo2017learning} discusses the emergence of cooperative and coordinated behaviors between joint and concurrent learning
agents using deep Q-learning. 
%
In the considered scenario, two agents form a team to win against an hard-coded AI,
and learn how to cooperate and how to divide the field.
%
As in \cite{mnih2013playing}, the images are resized to 84x84 and a frame skip of 4 is used.
Experience replay is also used.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{images/DQN_MAS.png}
  \caption{Multi-agent concurrent DQN \cite{diallo2017learning}.}
  \label{fig:dqnmas}
\end{figure}

There are three actions that each agent can take: move up, move down, stay at the same place.

\begin{table}[ht]
  \renewcommand{\arraystretch}{1.3}
  \caption{reward scheme adopted.}
  \label{tab:reward-scheme}
  \centering
  \begin{tabular}{@{}ccc@{}}
    \toprule
    \textbf{Events}            & \textbf{Agent 1 reward} & \textbf{Agent 2 reward} \\ \midrule
    \textbf{Left player loses} & +1                      & +1                      \\
    \textbf{One agent loses}   & -1                      & -1                      \\
    \textbf{Collision}         & -1                      & -1                      \\ \bottomrule
  \end{tabular}
\end{table}

Unlike previous works where the agent learns to play pong against a hard-coded AI, 
Tampuu, Ardi, et al. \cite{tampuu2017multiagent} have considered both as learning agents and have shown that different rewarding schemes lead them towards competition or collaboration.
%
Multiple agents controlled by autonomous DQNs learn to cooperate and compete while sharing a high-dimensional environment and being fed only raw visual input \cite{tampuu2017multiagent}.
%
Each of the two agents can take four actions: 
move up, move down, stand still, and fire (to relaunch the ball or to start the game).
%

\noindent
Three rewarding scheme were studied:
\begin{itemize}
  \item Score more than the opponent (fully competitive) $\rightarrow \rho = 1$.
  \item Loosing the ball penalizes both players (fully cooperative) $\rightarrow \rho = $ -1.
  \item Transition between cooperation and competition $\rightarrow \rho = range(-1, 1, 0.25)$.
\end{itemize}

\begin{table}[ht]
  \renewcommand{\arraystretch}{1.3}
  \caption{general rewarding scheme.}
  \label{tab:reward-scheme-2}
  \centering
  \begin{tabular}{c|c|c|}
    \cline{2-3}
                                                   & \textbf{L player scores} & \textbf{R player scores} \\ \hline
    \multicolumn{1}{|c|}{\textbf{L player scores}} & $\rho$                   & -1                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{R player scores}} & -1                       & $\rho$                   \\ \hline
  \end{tabular}
\end{table}

McBrien et al. \cite{mcbrien2020learning} proposed a different approach which uses a neural network automatically generated with genetic algorithms to allow agents to learn how to play Pong.
%
A Custom version of Pong is considered where the goals are smaller and the players have the ability to move in both the x and y dimensions.
%
The state is represented as eight inputs: the paddle's X and Y position, the opponent's X and Y position, the ball's X and Y velocity,
and the ball's X and Y position and the output of the neural network has four output nodes for each possible action: up, down, left and right.
%
Two algorithms were used: NeuroEvolution(NE) and NeuroEvolution of Augmenting Topologies(NEAT).
The first start from a predefined neural net structure and iteratively tune its parameters, while the second adds the possibility to modify the network topology by adding nodes and connections randomly.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{images/neuroevolution.png}
  \caption{NeuroEvolution algorithm schema.}
  \label{fig:ne}
\end{figure}
