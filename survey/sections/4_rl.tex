\section{Reinforcement Learning}
It is believed that Reinforcement Learning can provide a path towards generally capable agents \cite{silver2021reward} \cite{parker2022automated}.
It's very useful in today's world indeed it's widely used with self-driving cars, NLP, recommendation systems, financial trades, etc.

Reinforcement learning \cite{sutton2018reinforcement} describes a solution to a Markov Decision Process
with the goal of finding a policy that maximizes the sum of rewards \cite{huys2014reward}.
%
The agent learns an optimal (or nearly optimal) policy for the environment \cite{russell2002artificial}
through a series of reinforcements (rewards or penalties) that provide a quality of its behavior.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/RL_illustration.png}
    \caption{Simple diagram of the functioning of an RL system.}
    \label{fig:RL_illustration}
\end{figure}

\noindent
Reinforcement learning is divided in two approaches:
\begin{itemize}
    \item \textbf{Model-based}:
          The agent uses a transition model of the environment to help interpret the reward signals and to make decisions about how to act \cite{russell2021artificial}.
    \item \textbf{Model-free}:
          The agent learns the consequences of his actions through experience in order to refine his policy.
\end{itemize}

\subsection{Q-Learning}
Q-learning \cite{watkins1992q} is a model-free reinforcement learning algorithm which estimate a value for each (state, action) pair
and with these estimated values compute a policy that can maximize the expected discounted reward.

Given a set of states S, a set of actions A, a policy $\pi : S \rightarrow A$ and a reward function $R : A \times S \rightarrow \mathbb{R}$, the algorithm assigns a quality value to each (state, action) pair using the quality function $Q : S \times A \rightarrow \mathbb{R}$.
The quality values are updated iteratively at each agent action $a$ leading to a state $s'$ from a state $s$.
\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha [R(s, a) + \gamma \max_a Q(s', a) - Q(s, a)]
\end{equation}
where $\alpha \in [0, 1]$ is the learning rate and $\gamma \in [0, 1]$ is a discount factor which makes futures rewards less valuable than the current ones.

Given infinite exploration time the Q-learning algorithm is able to learn an optimal action-selection policy \cite{melo2001convergence}.

A problem of this method is the limitation of the state-action space required, that can be partially solved with an approximation function.
instead of storing each Q-values a mapping function could be learned to map a state-action pair to their respective Q-value.

\subsection{SARSA}
SARSA algorithm \cite{qiang2011reinforcement} is a variation of the Q-learning algorithm.
Its name come from $(s, a, r, s', a')$, that are (state, action, reward, next state, next action), and they are used to compute the update.

An algorithm has an "Off-Policy" technique if it learns the value function according to the action derived from another policy. On the other hand, it called "On-Policy" if it learns the value function according to the current action derived from the current policy.
Q-learning has an Off-Policy technique while SARSA has an On-Policy one.

The update formula of SARSA is similar to the one used by the Q-learning:
\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha [R(s, a) + \gamma Q(s', a') - Q(s, a)]
\end{equation}

This means that SARSA updates the state based on the action taken, while Q-learning updates following the optimal policy.
Suppose to be in a "cliff world", where the agent has to walk from the starting cell to the goal cell along the edge of a cliff without falling off. Q-learning, following the optimal policy, would tend to be close to the edge of the cliff while SARSA would prefer a "safer" path.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/cliff_word.png}
    \caption{Image to show the difference between SARSA and Q-learning in a "cliff word".}
\end{figure}

\section{Deep Reinforcement Learning}
When an MDP has a large number of states and actions, more memory and more time are required for learning.
To solve these problems it is possible to use approximation functions.
The function can be linear \cite{melo2008analysis} or non-linear as in the case of neural networks.

Deep reinforcement learning \cite{arulkumaran2017deep} combines reinforcement learning and deep learning and and represents policies as a neural network.

The deep learning enables RL to scale to decision-making problems that were previously intractable \cite{arulkumaran2017deep}.
Instead of a manual engineering representation of the state space, deep RL a very large and unstructured input data can be used.

A very big advantage that Deep RL offers instead of other input representation,
is that it can handle unseen states well \cite{mnih2013playing} \cite{karpathy2016deep}.

This is because a large portion of pixels could be similar to an image already seen and on which the model has been trained, so the network is likely to produce a similar prediction to the image seen previously. Instead, algorithms like MDP behave randomly in these cases.

\subsection{Deep Q-Network}
A Deep Q-Network (DQN) \cite{mnih2013playing} is a CNN adapted for RL used as a function approximator to estimate the Q-values, where the inputs are images and the outputs depends on the task's number of actions.
The loss is defined as the mean squared error between the Q-value target and the network's predicted at step $i$.
\begin{equation}
    L(\theta_i) \leftarrow \mathbb{E}_{s, a, r, s'} [((r + \gamma \max_{a'} Q(s', a'; \theta_i)) - Q(s, a; \theta_{i-1}))^2]
\end{equation}
where $s$ is the current state, $s'$ the next state, a the current action selected by the $\epsilon$-greedy policy, $r$ the immediate reward and $\theta$ are the network parameters.

To avoid computing the full expectation in the DQN loss, we can minimize it using stochastic gradient descent

Experience Replay is a technique introduced in The Atari DQN work to make the network updates more stable.
This method At each time step of data collection, add the transitions to a circular buffer called the replay buffer.

Given a random batch of transitions $(s, a, r, s')$ from the replay buffer we can calculate the loss at step $i$ as the following formula:
\begin{equation}
    L(\theta_i) \leftarrow ((r + \gamma \max_{a'} Q(s', a'; \theta_i)) - Q(s, a; \theta_{i-1}))^2
\end{equation}
