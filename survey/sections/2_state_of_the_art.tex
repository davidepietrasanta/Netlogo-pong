\section{State of The Art}
A learning agent is an agent who can operate in unfamiliar environments and can improve its knowledge by learning from experiences.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{images/IntelligentAgent-Learning.png}
    \caption{Learning agent.}
\end{figure}

Reinforcement Learning is about use observed rewards to learn an optimal (or nearly optimal)
policy for the environment \cite{russell2002artificial}. In other words, it is maximizing an
agent's reward by performing a series of actions in response to a dynamic environment.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/RL_illustration.png}
    \caption{Simple diagram of the functioning of an RL system.}
    \label{fig:RL_illustration}
\end{figure}

Reinforcement learning allows an agent to learn through a series of reinforcements (rewards or penalties) that provide a quality of its behavior.
The goal is to maximize the sum of rewards of the agent.
Reinforcement learning is divided in two approaches: Model-Based and Model-Free.

\subsection{Model Based Reinforcement Learning}
Defines a model of the environment and learn its representation

\subsection{Model Free Reinforcement Learning}
Defines a mapping for each state action called policy

\begin{itemize}
    \item action-utility learning
    \item policy-search
\end{itemize}

\subsubsection{Q-Learning}
\subsubsection{SARSA}
SARSA algorithm \cite{qiang2011reinforcement} is a variation of the Q-learning algorithm.
Its name come from $(s, a, r, s', a')$, that are $(state, action, reward, state', action')$, and they are used to compute the update.

An algorithm has an "Off-Policy" technique if it learns the value function according to the action derived from another policy. On the other hand, it called "On-Policy" if it learns the value function according to the current action derived from the current policy.
Q-learning has an Off-Policy technique while SARSA has an On-Policy one.

The update formula of SARSA is similar to the one used by the Q-learning: $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a) - Q(s_t, a_t)]$.

This means that SARSA updates the state based on the action taken, while Q-learning updates following the optimal policy.
Suppose to be in a "cliff world", where the agent has to walk from the starting cell to the goal cell along the edge of a cliff without falling off. Q-learning, following the optimal policy, would tend to be close to the edge of the cliff while SARSA would prefer a "safer" path.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/cliff_word.png}
    \caption{Image to show the difference between SARSA and Q-learning in a "cliff word".}
\end{figure}

\subsubsection{Temporal Difference}

\subsection{Deep Reinforcement Learning}
Deep Q Networks(DQN) were introduced in "Playing Atari with Deep Reinforcement Learning" \cite{mnih2013playing}.
A DQN is a CNN adapted for RL used as a function approximator to estimate the Q-values, where the inputs are images and the outputs depends on the task's number of actions.
The loss is defined as the mean squared error between the Q-value target and the network's predicted at step $i$.

$L(\theta_i) \leftarrow \mathbb{E}_{s, a, r, s'} [((r + \gamma \max_{a'} Q(s', a'; \theta_i)) - Q(s, a; \theta_{i-1}))^2]$
where s is the current state, s' the next state, a the current action selected by the $\epsilon$-greedy policy, r the immediate reward and $\theta$ are the network parameters.

To avoid computing the full expectation in the DQN loss, we can minimize it using stochastic gradient descent

Experience Replay is a technique introduced in The Atari DQN work to make the network updates more stable.
This method At each time step of data collection, add the transitions to a circular buffer called the replay buffer.

Given a random batch of transitions $(s, a, r, s')$ from the replay buffer we can calculate the loss at step $i$ as the following formula
$L(\theta_i) \leftarrow ((r + \gamma \max_{a'} Q(s', a'; \theta_i)) - Q(s, a; \theta_{i-1}))^2$

Better describe the stability problem of DQN... Non-stationary or unstable target


\subsection*{Learning Agents for Pong}

After papers like "Reward is enough"\cite{silver2021reward} and "Playing Atari with Deep Reinforcement Learning"\cite{mnih2013playing} the enthusiasm for the RL has risen more and more.
"Reward is enough" hypothesizes how intelligence can be understood as subservient to reward maximization. The reward is enough to drive behavior that displays skills studied in natural and artificial intelligence. This is in contrast to the idea that specialized problem formulations, based on other signals or goals, are required for each skill \cite{silver2021reward}.
"Playing Atari with Deep Reinforcement Learning" presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory inputs (raw pixels) using Reinforcement Learning \cite{mnih2013playing}.

In this work seven popular ATARI games were considered including pong.

Game frames was captured at a resolution of 210x160 gray-scale, down-sampling it to a 110x84 image, cropped to a 84x84 patch of the playing area
and given in input to a modified version of a CNN for RL (Deep Q Network).

A frame skipping technique is used to play roughly k times more games without significantly increasing the runtime.

The network has an input shape of 84x84x4 and has a separate output unit for each possible action, the number of which depends on the games considered (between 4 and 18 in this case).
For all the games considered all positive rewards are fixed to be 1 and all negative rewards to be -1.

A variant of online Q-Learning that combines stochastic mini-batch updates with experience replay memory was used to ease the training of deep RL networks.
The network was trained with an $\epsilon$-greedy annealed strategy for a total of 10 million frames and a replay memory of one million most recent frames was used.




Another work that use DQN is "Learning to Play Pong Video Game via Deep Reinforcement Learning" where they focus on pong only.
The reward considered is the same as in "Playing Atari with Deep Reinforcement Learning" +1 when the agent score a point -1 when the opponent score a point.
The screenshots with a fixed 32 FPS are binarized and rescaled to 80x80.
Agent actions considered are move paddle up, move paddle down, paddle stays at the same place.
In particular they added an Episodic Control technique, implemented with an embedding function represented as a random projection, to reuse successful strategies.




The "Learning to Coordinate with Deep Reinforcement Learning in Doubles Pong Game" discusses the emergence of cooperative and coordinated behaviors between joint and concurrent learning
agents using deep Q-learning, by considering a scenario where 2 agents DQN`s form a team to win against an hard-coded AI,
and automatically learn how to cooperate and divide the space of their side of the field.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{images/DQN_MAS.png}
    \label{dqnmas}
    \caption{DQN adapted to MAS.}
\end{figure}

There are three actions that each agent can take: move up, move down, stay at the same place.

\begin{table}[ht]
    \centering
    \begin{tabular}{@{}ccc@{}}
        \toprule
        \textbf{Events}            & \textbf{Agent 1 reward} & \textbf{Agent 2 reward} \\ \midrule
        \textbf{Left player loses} & +1                      & +1                      \\
        \textbf{One agent loses}   & -1                      & -1                      \\
        \textbf{Collision}         & -1                      & -1                      \\ \bottomrule
    \end{tabular}
    \label{my-table}
    \caption{reward scheme adopted.}
\end{table}

Instead of randomly sampling prioritized experience replay is used.




All previous works make the agent learn to play pong against an hardcoded AI agent, but "Multi agent cooperation and competition with deep reinforcement learning" considered to use an opponent
which is Learning too and showed that this can further improve the robustness of the model when playing with others different opponent.

Multiple agents controlled by autonomous DQNs learn to cooperate and compete while sharing a high-dimensional environment and being fed only raw visual input
There are 4 actions that each of the two agents can take: move up, move down, stand still, and fire (to relaunch the ball or to start the game).

\noindent
3 rewarding scheme:
\begin{itemize}
    \item Score more than the opponent (fully competitive) $\rightarrow \rho = 1$.
    \item Loosing the ball penalizes both players (fully cooperative) $\rightarrow \rho = $ -1.
    \item Transition between cooperation and competition $\rightarrow \rho = range(-1, 1, 0.25)$.
\end{itemize}

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|}
        \cline{2-3}
                                                       & \textbf{L player scores} & \textbf{R player scores} \\ \hline
        \multicolumn{1}{|c|}{\textbf{L player scores}} & $\rho$                   & -1                       \\ \hline
        \multicolumn{1}{|c|}{\textbf{R player scores}} & -1                       & $\rho$                   \\ \hline
    \end{tabular}
    \caption{general rewarding scheme.}
    \label{tab:my-table2}
\end{table}


A different is "LEARNING PONG" which uses a neural network automatically generated with genetic algorithms to let the agents learn to play Pong, in this case no scripted opponent AI.

A Custom version of Pong is considered where the goals are smaller and the players have the ability to move in both the x and y dimensions.

The state is represented as eight inputs: the paddle's X and Y position, the opponent's X and Y position, the ball's X and Y velocity,
and the ball's X and Y position and the output of the neural network has four output nodes: to movement up, down, left and right.

Two algorithms were used the NeuroEvolution(NE) and NeuroEvolution of Augmenting Topologies(NEAT).
The first start from a predefined neural net structure and iteratively tune its parameters, while the second adds the possibility to modify the network topology by adding nodes and connections randomly.



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{images/neuroevolution.png}
    \label{ne}
    \caption{NeuroEvolution algorithm schema.}
\end{figure}

"Playing Atari with Deep Reinforcement Learning" \cite{mnih2013playing} and "Deep Reinforcement Learning: Pong from Pixels" \cite{karpathy2016deep} show how applying Deep Reinforcement Learning for the game of pong leads to the best results.\\

The policy function, or policy network, used to decide what to do based on the current state, is a fully connected neural network with n hidden layers, which is why this method is called Deep RL. In the papers frame image of the game is taken as input, in which each pixel corresponds to a single input neuron, and returns a number between 0 and 1 which can be seen as the probability to win with a given action (e.g. left). If the number is greater than 0.5 we go to the left, otherwise we go to the right. If it is 0.5, a random choice is made. To do this, the output neuron has a sigmoid function \cite{mnih2013playing}\cite{karpathy2016deep}.
Since politics generates probability, this politics is stochastic.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{images/DRL_network.png}
    \caption{Simple image of the policy network of an Deep RL system.}
    \label{fig:DRL_network}
\end{figure}

This can also be easily applied to inputs of a different nature, such as vectors representing the position of the ball, the direction, the position of the paddles, etc.

A very big advantage that Deep RL offers is that it can handle unseen states well \cite{mnih2013playing} \cite{karpathy2016deep}.
This is because a large portion of pixels could be similar to an image already seen and on which the NN has been trained, so the network is likely to produce a similar prediction to the image seen previously. Instead, algorithms like MDP behave randomly in these cases. \\

Generally, to facilitate learning, pre-processing is applied.
An image can then be cropped to eliminate those pixels that are not needed for prediction, such as the scoreboard. Other things that can help are grayscaling the image and reducing image resolution \cite{mnih2013playing}.
