\section{Markov Decision Process (MDP)}
\label{sec:mdp}
A sequential decision problem for a fully observable stochastic environment with a 
Markovian transition model and additive rewards is called a 
Markov Decision Process or MDP \cite{russell2002artificial}.
It consists of a set of states S, a set of actions A(s) for each state, a transition model
$P(s' | s, a)$ and a reward function R(s).

Due to the stochastic natures of the environment, the quality of a policy is measured by the expected utility of the possible environmental histories generated by that policy.

This approach is very simple and can lead to several problems.
If the space of actions and states is vast, there will be problems in learning.
Moreover, if a state has never been seen the next action will be random and this is more
likely with large spaces, due to the sparsity of the transition matrix \cite{silver2015}.
This method can also bring with it the rewards' sparseness problem, which occurs when many
actions take place between one reward and another. This is called Temporal Credit Assignment Problem \cite{sutton1984temporal}.
However the reward function depends on only the current state, it is
therefore difficult to integrate a discounted rewards, which aims to reward/penalize
more actions close to victory/defeat, without considering an history and therefore
without using Hidden Markov Models \cite{silver2015}.

As already mentioned, for unseen states we cannot precisely predict the reward.
Q-Learning is an algorithm that can be used to solve MDP problems with unknown reward.
It also manages to solve the integration of the discounted rewards easily.
