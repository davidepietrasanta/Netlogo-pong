\section{Markov Decision Process (MDP)}

A sequential decision problem for a fully observable stochastic environment with a Markovian transition model and additive rewards is called a Markov Decision Process or MDP \cite{russell2002artificial}.
It consists of a set of states S, a set of actions A(s) for each state, a transition model
$P(s' | s, a)$ and a reward function R(s).

Due to the stochastic natures of the environment, the quality of a policy is measured by the expected utility of the possible environmental histories generated by that policy. An optimal policy is a policy that yields the highest expected utility.

This approach can also be applied to Pong.
Considering the Markov hypothesis, which states that the future states of the process depend only on the present state, we can say that the probability of winning with an action a, given a state s, is:

\[ { Map[s][a] = \frac{ Count(a_{won} | s) }{ Count(a_{won} + a_{lost} | s) } } \]

Where $a_{won}$ are the times that action a in state s led to a victory and
$a_{lost}$ are the times that action a in state s led to a defeat.
We then created a map between actions and states. \\

This approach is very simple and can lead to several problems. \\
If the space of actions and states is vast, there will be problems in learning.
Moreover, if a state has never been seen the next action will be random and this is more
likely with large spaces, due to the sparsity of the transition matrix \cite{silver2015}.
This method can also bring with it the rewards' sparseness problem, which occurs when many
actions take place between one reward and another. This is called Temporal Credit Assignment Problem \cite{sutton1984temporal}
and fortunately, Pong doesn't suffer much from this problem, like chess or go does.
However the reward function depends on only the current state, it is
therefore difficult to integrate a discounted rewards, which aims to reward/penalize
more actions close to victory/defeat, without considering a story and therefore
without using Hidden Markov Models \cite{silver2015}.

As already mentioned, for unseen states we cannot precisely predict the reward.
Q-Learning is an algorithm that can be used to solve MDP's problem with unknown reward.
It also manages to solve the integration of the discounted rewards easily.
